{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6459d6-6274-4cf1-b4db-52450dfc7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory=MemorySaver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dccc569-1348-45ac-9d32-3b7ad7922d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb695157-a279-41db-9238-e4adeedaa7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass(\"ðŸ” Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200e3b1-a5d6-407b-b7a5-667e0bc4e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We want to keep track of \n",
    "task: for human input\n",
    "plan: plan the planning agent will generate\n",
    "draft: draft of the essay\n",
    "critique: Populated critique agent\n",
    "content: tracking list of documents that Tavily has come back with\n",
    "revison number: keeps track of number of revisions we have made\n",
    "max_revisions: to decide on number of revisions\n",
    "we use critique , revision number and max_revisions to decide on when to stop\n",
    "\n",
    "'''\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e53cc-3fbe-4e4c-a898-9b14f93fe61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701f632-58a8-4a9e-b9a7-4ed9c5acecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for the llm that will give a plan for the essay\n",
    "\n",
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bef75-0ef8-4fab-897a-3bef810d6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will write the essay given all the content that was researched\n",
    "\n",
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3d3e5-a9ea-4b9d-a2b9-b0d076a5b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Controls how we are critiquing the draft of the essay\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f6f68-5daa-4c8f-81db-16e05b8dddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a plan we pass queries to Tavilly\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fc1f3-07ea-49a9-a84c-02456bddb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It works on a critique\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf04a33-ba14-4162-b68a-d71574954b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field  \n",
    "# We want to get a list of strings from llm \n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6f125-4a42-4c30-a273-b4e7daac4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291fd8e-960b-45ad-abe7-19d9900a3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating all the nodes\n",
    "\n",
    "#Passing the system prompt and human prompt to the plan node\n",
    "\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068709d9-0847-4085-8234-ab7210b7b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This node takes in the plan and does initial research using Tavily\n",
    "#Output is the list of content that we are going to use to write the essay.\n",
    "\n",
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state.get(\"content\", [])\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e018f34-4318-4c57-9f5f-f382f326ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now have the plan and content. This node will write a draft\n",
    "#The response will be a draft\n",
    "\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221ed4a-eebe-47ce-93aa-489e447fe46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reflection will take in the draft, reflection prompt and will generate the critique\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb308d3-9439-4368-8c9e-a111ce2d264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in the critique from reflection node,we update the content\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e501ace-2675-4634-9e52-7894cd91481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ends if > than the max_revisions\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea805d-17fe-4e86-a73f-4ceb96638a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the graph\n",
    "builder = StateGraph(AgentState)# Initialize the graph with agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685eeac-7d8b-4a1e-9a24-51b1a30d8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding nodes\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d71f1-4ab5-4b92-a6db-461ec63f84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"planner\") #Initial entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea7bbb-a53b-4663-bc50-0fa109de23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding conditional edge after generate. We either reflect or end.On reflect we go to the reflect node.\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d615c48-c21e-443e-a6cf-cbed236592da",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Adding edges\n",
    "After planning we go to research plan , theen generate, then reflect ->research critique\n",
    "'''\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63618888-963f-481d-9295-ec57491d047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ee4fe-2ad3-42a5-ab30-08d15212b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Essay writer\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "}, thread):\n",
    "    print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aad5d3-6eea-40b9-bc0d-a643fc490bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional to view the graph\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
